{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"JPI","text":"<p>JPI (JAX Parallel Interface) is a library for distributed computing with JAX using MPI. It provides composable primitives for parallel operations that integrate seamlessly with JAX's transformation system, enabling efficient distributed scientific computing.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>MPI collective operations: Implementations of <code>allreduce</code>, <code>allgather</code>, <code>scatter</code>, <code>gather</code>, <code>broadcast</code>, and <code>barrier</code>. More can easily be added.</li> <li>JAX transformation: Full support for <code>jit</code>, <code>grad</code>, <code>vmap</code>, and other JAX transformations</li> <li>Automatic differentiation: Custom VJP definitions ensure correct gradient computation through parallel operations</li> <li>Interoperability with mpi4py: Uses <code>mpi4py</code> communicators, allowing easy integration with existing MPI-based codebases.</li> <li>Token-based synchronization: As JAX and XLA operate on the assumption that all primitives are pure functions without side effects, the compiler is in principle free to re-order operations. Inspired by mpi4jax, this is handeled by introducing a fake data dependency between subsequent calls using tokens.</li> <li>JAX FFI backend: The MPI operations are implemented in C++ and interfaced with JAX using the Foreign Function Interface (FFI) for performance. There is no copying of data between JAX and the MPI backend, ensuring low overhead.</li> </ul>"},{"location":"#current-limitations","title":"Current Limitations","text":"<ul> <li>CPU-only support: GPU operations are not yet implemented. However, this can be added by implementing FFI calls on GPU as described in the FFI documentation.</li> <li>Limited MPI operations: Only a subset of MPI collective operations are implemented.</li> <li>Development stage: API may change in future versions</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Installing currently requires some system-level dependencies. Make sure these are installed:</p> <ul> <li><code>uv</code>: Recommended package manager. See Installing uv.</li> <li><code>Python3 &gt;=3.13</code>: See Installing python for installing Python using <code>uv</code>.</li> <li><code>git</code>: See git downloads.</li> <li><code>OpenMPI</code>: See OpenMPI documentation. You might need to update the <code>CMakeLists.txt</code> file to point to the correct MPI installation.</li> </ul>"},{"location":"#building-the-project","title":"Building the project","text":"<p>As the MPI operations are implemented in C++, the project needs to be built before use. This is handled automatically when installing with <code>uv</code>. Install the project with:</p> <p><pre><code># Clone the repository\ngit clone https://github.com/ellingsvee/jpi.git\ncd jpi\n\n# Install with uv\nuv sync\nuv build\n</code></pre> If the installation fails, it could be that your MPI-installation is not found. This might need to be specified in the <code>CMakeLists.txt</code> file.</p>"},{"location":"#modifying-the-c-backend","title":"Modifying the C++ backend","text":"<p>If you make changes to the C++ code, you need to rebuild the project. This can be done with:</p> <pre><code>uv sync --reinstall\nuv build\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom mpi4py import MPI\nfrom jpi import allreduce, scatter, gen_token\n\n# Use mpi4py communicator\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nsize = comm.Get_size()\n\n# Fill the root rank with data, others with zeros\nif rank == 0:\n    x = jnp.arange(2 * size, dtype=jnp.float32)\nelse:\n    x = jnp.zeros(2 * size, dtype=jnp.float32)\n\n\n# Some example function that uses scatter and allreduce\ndef func(x):\n    # Generate token for synchronization between operations\n    token = gen_token()\n\n    # Scatter x from rank 0 to all ranks\n    x, token = scatter(x, token, root=0, comm=comm)\n\n    # Each rank can do something different with the array\n    if rank == size - 1:\n        x = x * 2\n\n    # Perform allreduce (sum) on the scattered array\n    result, token = allreduce(x, token, op=MPI.SUM, comm=comm)\n    return jnp.sum(result)\n\n\n# JIT and grad the function\nfunc_jit = jax.jit(func)\nfunc_grad = jax.grad(func)\n\n# Compute result and gradient\nresult = func_jit(x)\ngrad_result = func_grad(x)\nprint(f\"Rank {comm.rank} has result {result} and gradient {grad_result}\")\n\n# Out\n# Rank 0 has result 41.0 and gradient [1. 1. 1. 1. 1. 1. 2. 2.]\n# Rank 1 has result 41.0 and gradient [0. 0. 0. 0. 0. 0. 0. 0.]\n# Rank 2 has result 41.0 and gradient [0. 0. 0. 0. 0. 0. 0. 0.]\n# Rank 3 has result 41.0 and gradient [0. 0. 0. 0. 0. 0. 0. 0.]\n</code></pre> <p>Run the above code with MPI using:</p> <pre><code>mpirun -np 4 uv run examples/intro_example.py\n</code></pre>"},{"location":"#testing","title":"Testing","text":"<p>Tests are implemented using <code>pytest</code>. To run the tests with MPI use:</p> <pre><code>mpirun -np 4 uv run pytest --with-mpi \n</code></pre>"},{"location":"#license","title":"License","text":"<p>MIT License. See <code>LICENSE</code> file for details.</p>"},{"location":"#alternatives","title":"Alternatives","text":"<p>This project is inspired by the great mpi4jax.  Built using <code>mpi4py.libmpi</code> to exposes MPI C primitives as Cython callables, mpi4jax is currently more mature and has more features. JPI aims to provide a simpler and more extensible framework for integrating MPI with JAX.  Additionally, building on top of JAX's FFI allows XLA to better optimize the C++ backend for performance.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>jpi<ul> <li>allgather</li> <li>allreduce</li> <li>barrier</li> <li>bcast</li> <li>comm</li> <li>gather</li> <li>op</li> <li>scatter</li> <li>token</li> </ul> </li> </ul>"},{"location":"reference/jpi/","title":"jpi","text":""},{"location":"reference/jpi/#jpi","title":"<code>jpi</code>","text":""},{"location":"reference/jpi/#jpi-functions","title":"Functions","text":""},{"location":"reference/jpi/#jpi.gen_token","title":"<code>gen_token()</code>","text":"<p>Generate a synchronization token for MPI operations.</p> <p>Returns:</p> Type Description <code>Token</code> <p>A scalar JAX array that serves as a synchronization token.</p> Note <p>Tokens should be threaded through MPI operations to maintain proper ordering. Each MPI operation consumes a token and produces a new one.</p> Example <pre><code>from jpi import bcast, allreduce, gen_token\n\n# Create initial token\ntoken = gen_token()\n\n# Thread token through operations\nresult1, token = bcast(data1, token, root=0)\nresult2, token = allreduce(data2, token, MPI.SUM)\n</code></pre>"},{"location":"reference/jpi/allgather/","title":"allgather","text":""},{"location":"reference/jpi/allgather/#jpi.allgather","title":"<code>allgather</code>","text":""},{"location":"reference/jpi/allgather/#jpi.allgather-functions","title":"Functions","text":""},{"location":"reference/jpi/allgather/#jpi.allgather.allgather","title":"<code>allgather(x, token, comm=None)</code>","text":"<p>Gather arrays from all processes and distribute to all.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Local array to contribute to the gather operation. Must have the same shape on all processes except possibly the first dimension.</p> required <code>token</code> <code>Token</code> <p>Synchronization token for ordering operations.</p> required <code>comm</code> <code>Comm | None</code> <p>MPI communicator. If None, uses the default communicator.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Array</code> <p>Concatenated array with shape (total_elements, *x.shape[1:]), where total_elements = sum of x.shape[0] across all processes.</p> <code>new_token</code> <code>Token</code> <p>Updated synchronization token.</p> Example <pre><code>import jax.numpy as jnp\nfrom jpi import allgather\nfrom jpi import gen_token\n\n# Each rank contributes different data\nlocal_data = jnp.array([rank, rank + 1])  # rank-specific data\ntoken = gen_token()\nresult, token = allgather(\n    local_data, token\n)  # result contains data from all ranks concatenated\n</code></pre>"},{"location":"reference/jpi/allreduce/","title":"allreduce","text":""},{"location":"reference/jpi/allreduce/#jpi.allreduce","title":"<code>allreduce</code>","text":""},{"location":"reference/jpi/allreduce/#jpi.allreduce-functions","title":"Functions","text":""},{"location":"reference/jpi/allreduce/#jpi.allreduce.allreduce","title":"<code>allreduce(x, token, op, comm=None)</code>","text":"<p>Perform a reduction operation across all processes.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Local array to contribute to the reduction.</p> required <code>token</code> <code>Token</code> <p>Synchronization token for ordering operations.</p> required <code>op</code> <code>Op</code> <p>MPI reduction operation. Supported operations include: - MPI.SUM: Element-wise sum across all processes - MPI.PROD: Element-wise product across all processes - MPI.MAX: Element-wise maximum across all processes - MPI.MIN: Element-wise minimum across all processes</p> required <code>comm</code> <code>Comm | None</code> <p>MPI communicator. If None, uses the default communicator.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Array</code> <p>Array containing the reduction result (same on all processes).</p> <code>new_token</code> <code>Token</code> <p>Updated synchronization token.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the backward pass is not implemented for the specified reduction operation.</p> Example <pre><code>import jax.numpy as jnp\nfrom jpi import allreduce, gen_token\nfrom mpi4py import MPI\n\n# Sum arrays across all processes\nlocal_data = jnp.array([1.0, 2.0]) * (rank + 1)\ntoken = gen_token()\nresult, token = allreduce(\n    local_data, token, MPI.SUM\n)  # result contains the sum from all processes\n</code></pre>"},{"location":"reference/jpi/barrier/","title":"barrier","text":""},{"location":"reference/jpi/barrier/#jpi.barrier","title":"<code>barrier</code>","text":""},{"location":"reference/jpi/barrier/#jpi.barrier-functions","title":"Functions","text":""},{"location":"reference/jpi/barrier/#jpi.barrier.barrier","title":"<code>barrier(token, comm=None)</code>","text":"<p>Synchronize all processes in the communicator.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>Token</code> <p>Synchronization token for ordering operations. The token is passed through unchanged but ensures proper sequencing.</p> required <code>comm</code> <code>Comm | None</code> <p>MPI communicator. If None, uses the default communicator.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>new_token</code> <code>Token</code> <p>Updated synchronization token (same value as input).</p> Example <pre><code>import jax.numpy as jnp\nfrom jpi import barrier, gen_token\n\n# Ensure all processes reach this point before continuing\ntoken = gen_token()\ntoken = barrier(token)  # Now all processes have synchronized\n</code></pre>"},{"location":"reference/jpi/bcast/","title":"bcast","text":""},{"location":"reference/jpi/bcast/#jpi.bcast","title":"<code>bcast</code>","text":""},{"location":"reference/jpi/bcast/#jpi.bcast-functions","title":"Functions","text":""},{"location":"reference/jpi/bcast/#jpi.bcast.bcast","title":"<code>bcast(x, token, root, comm=None)</code>","text":"<p>Broadcast an array from one process to all others.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input array to broadcast. Only meaningful on the root process.</p> required <code>token</code> <code>Token</code> <p>Synchronization token for ordering operations.</p> required <code>root</code> <code>int</code> <p>Rank of the root process that owns the data to broadcast.</p> required <code>comm</code> <code>Comm | None</code> <p>MPI communicator. If None, uses the default communicator.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Array</code> <p>The broadcasted array (same on all processes).</p> <code>new_token</code> <code>Token</code> <p>Updated synchronization token.</p> Example <pre><code>import jax.numpy as jnp\nfrom jpi import bcast, gen_token\n\n# On rank 0: broadcast this data\nif rank == 0:\n    data = jnp.array([1.0, 2.0, 3.0])\nelse:\n    data = jnp.zeros(3)  # Will be overwritten\n\ntoken = gen_token()\nresult, token = bcast(\n    data, token, root=0\n)  # Now all processes have [1.0, 2.0, 3.0]\n</code></pre>"},{"location":"reference/jpi/comm/","title":"comm","text":""},{"location":"reference/jpi/comm/#jpi.comm","title":"<code>comm</code>","text":""},{"location":"reference/jpi/comm/#jpi.comm-functions","title":"Functions","text":""},{"location":"reference/jpi/comm/#jpi.comm.get_default_comm","title":"<code>get_default_comm()</code>","text":"<p>Get the default MPI communicator (MPI.COMM_WORLD).</p>"},{"location":"reference/jpi/gather/","title":"gather","text":""},{"location":"reference/jpi/gather/#jpi.gather","title":"<code>gather</code>","text":""},{"location":"reference/jpi/gather/#jpi.gather-functions","title":"Functions","text":""},{"location":"reference/jpi/gather/#jpi.gather.gather","title":"<code>gather(x, token, root, comm=None)</code>","text":"<p>Gather arrays from all processes.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Local array to contribute to the gather operation. Must have the same shape on all processes except possibly the first dimension.</p> required <code>token</code> <code>Token</code> <p>Synchronization token for ordering operations.</p> required <code>root</code> <code>int</code> <p>Rank of the root process that will receive the gathered data.</p> required <code>comm</code> <code>Comm | None</code> <p>MPI communicator. If None, uses the default communicator.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Array</code> <p>Concatenated array with shape (total_elements, *x.shape[1:]), where total_elements = sum of x.shape[0] across all processes.</p> <code>new_token</code> <code>Token</code> <p>Updated synchronization token.</p> Example <pre><code>import jax.numpy as jnp\nfrom jpi import gather, gen_token\n\n# Each rank contributes different data\nlocal_data = jnp.array([rank, rank + 1])  # rank-specific data\ntoken = gen_token()\nresult, token = gather(\n    local_data, token, root=0\n)  # rank 0 contains data from all ranks concatenated\n</code></pre>"},{"location":"reference/jpi/op/","title":"op","text":""},{"location":"reference/jpi/op/#jpi.op","title":"<code>op</code>","text":""},{"location":"reference/jpi/scatter/","title":"scatter","text":""},{"location":"reference/jpi/scatter/#jpi.scatter","title":"<code>scatter</code>","text":""},{"location":"reference/jpi/scatter/#jpi.scatter-functions","title":"Functions","text":""},{"location":"reference/jpi/scatter/#jpi.scatter.scatter","title":"<code>scatter(x, token, root, comm=None)</code>","text":"<p>Distribute arrays to all processes.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Local array to contribute to the scatter operation. Must have the same shape on all processes except possibly the first dimension.</p> required <code>token</code> <code>Token</code> <p>Synchronization token for ordering operations.</p> required <code>root</code> <code>int</code> <p>Rank of the root process that will distribute the data.</p> required <code>comm</code> <code>Comm | None</code> <p>MPI communicator. If None, uses the default communicator.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Array</code> <p>Sliced array with shape (x.shape[0] // size, *x.shape[1:]), where size is the number of processes.</p> <code>new_token</code> <code>Token</code> <p>Updated synchronization token.</p> Example <pre><code>import jax.numpy as jnp\nfrom jpi import scatter, gen_token\n\n# Each rank contributes different data\nlocal_data = jnp.array([rank, rank + 1])  # rank-specific data\ntoken = gen_token()\nresult, token = scatter(local_data, token, root=0)\n</code></pre>"},{"location":"reference/jpi/token/","title":"token","text":""},{"location":"reference/jpi/token/#jpi.token","title":"<code>token</code>","text":""},{"location":"reference/jpi/token/#jpi.token-functions","title":"Functions","text":""},{"location":"reference/jpi/token/#jpi.token.gen_token","title":"<code>gen_token()</code>","text":"<p>Generate a synchronization token for MPI operations.</p> <p>Returns:</p> Type Description <code>Token</code> <p>A scalar JAX array that serves as a synchronization token.</p> Note <p>Tokens should be threaded through MPI operations to maintain proper ordering. Each MPI operation consumes a token and produces a new one.</p> Example <pre><code>from jpi import bcast, allreduce, gen_token\n\n# Create initial token\ntoken = gen_token()\n\n# Thread token through operations\nresult1, token = bcast(data1, token, root=0)\nresult2, token = allreduce(data2, token, MPI.SUM)\n</code></pre>"}]}